# Physical AI & Humanoid Robotics ü¶æü§ñ

A **spec-driven, AI-authored technical book** focused on **Physical AI and Humanoid Robotics**, built using **Spec-Kit Plus**, **Claude Code**, and **Docusaurus**, and deployed via **vercel**.
<img width="1366" height="768" alt="Screenshot (205)" src="https://github.com/user-attachments/assets/2402c4d5-b5ed-465d-98f6-c638fc45a2b2" />

<img width="1366" height="768" alt="Screenshot (202)" src="https://github.com/user-attachments/assets/d0d93a32-681d-477e-a8d5-1a55c61bae03" />



This project explores how AI systems move beyond digital environments into the **physical world**, enabling humanoid robots to perceive, reason, and act using embodied intelligence.

üåê Live Book:  
https://humanoid-robotics-book-7mzo.vercel.app/

---

## üìò Project Overview

The future of AI lies in **Physical AI**‚Äîsystems that understand physics, interact with real environments, and operate through robotic bodies.

This book bridges the gap between:
- **Digital intelligence (LLMs, planners, perception)**
- **Physical embodiment (robots, sensors, actuators)**

Students and developers learn how to design, simulate, and deploy **autonomous humanoid robots** using modern robotics and AI tooling.

---

## üéØ Focus Areas

- Embodied Intelligence & Physical AI
- ROS 2 (Robot Operating System)
- Gazebo & Unity Digital Twins
- NVIDIA Isaac Sim & Isaac ROS
- Vision-Language-Action (VLA)
- Conversational Robotics (Whisper + LLMs)
- Sim-to-Real Robot Deployment

---

## üß© Book Structure

### Module 1: The Robotic Nervous System (ROS 2)
- ROS 2 nodes, topics, services, actions
- Python agents with `rclpy`
- URDF humanoid robot modeling

### Module 2: The Digital Twin (Gazebo & Unity)
- Physics simulation (gravity, collisions)
- Sensor simulation (LiDAR, depth cameras, IMUs)
- High-fidelity robot environments

### Module 3: The AI-Robot Brain (NVIDIA Isaac)
- Isaac Sim & synthetic data generation
- Isaac ROS for VSLAM and navigation
- Path planning with Nav2

### Module 4: Vision-Language-Action (VLA)
- Voice-to-Action using OpenAI Whisper
- LLM-based cognitive planning
- Multi-modal human-robot interaction

### Capstone: Autonomous Humanoid
- Natural language command input
- Path planning & navigation
- Object recognition & manipulation
- End-to-end autonomous behavior

---

## üõ†Ô∏è Tech Stack

- **Documentation:** Docusaurus + Markdown
- **Spec-Driven Authoring:** Spec-Kit Plus
- **AI Authoring:** Claude Code
- **Robotics:** ROS 2, Gazebo, Unity
- **AI & Simulation:** NVIDIA Isaac Sim, Isaac ROS
- **Edge AI:** Jetson Orin Nano / NX
- **Deployment:** GitHub Pages

---

## üß™ Development Workflow

This project follows a **Spec-Driven Development** process:

1. `/sp.constitution` ‚Äì Core principles & standards  
2. `/sp.spec` ‚Äì Project scope & requirements  
3. `/sp.clarify` ‚Äì Resolve ambiguities  
4. `/sp.plan` ‚Äì Architecture & validation strategy  
5. `/sp.tasks` ‚Äì Execution breakdown  
6. AI-assisted writing + validation  
7. Docusaurus build & deployment  

---

## üöÄ Getting Started (Local)

```bash
git clone https://github.com/ersa-rani/humanoid-robotics-book.git
cd humanoid-robotics-book
npm install
npm run start
Build for production:

bash
Copy code
npm run build
üéì Target Audience
AI & Robotics students

Computer Science learners

Hackathon participants

Developers exploring humanoid robotics

Anyone interested in Physical AI & embodied intelligence

üìú License
This project is released for educational and research purposes.


üôå Acknowledgements
Spec-Kit Plus (Panaversity)

gemini cli

ROS 2 Community

NVIDIA Isaac Platform

OpenAI (Whisper & LLM concepts)

‚ú® Author
Ersa Rani
